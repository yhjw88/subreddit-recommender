{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import cPickle as pickle\n",
    "import itertools\n",
    "import gc\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pixie\n",
    "import random\n",
    "import snap\n",
    "import time\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixie does not work well if we do not bias the walk. In order to do so we need edges to be weighted, and the graph is simply too large to fit into RAM if we were to incorporate all the features.\n",
    "\n",
    "Instead, we will prune out edges with weight 1. Experiments show that this has minimal sacrifice in metrics, and we will hopefully be able to improve results by incorporating more features into the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ids to Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by generating two lists to map ids to indexes, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Users: 6283057, Subreddits: 152976\n"
     ]
    }
   ],
   "source": [
    "# Get the set of necessary users and subreddits.\n",
    "users = set()\n",
    "subreddits = set()\n",
    "with open(\"../bigData/finalGeneration/inputGraph\", 'r') as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        userId, subredditId, commentCountStr = line.split()\n",
    "        commentCount = int(commentCountStr)\n",
    "        if commentCount > 0:\n",
    "            users.add(userId)\n",
    "            subreddits.add(subredditId)\n",
    "\n",
    "        if i % 1000000 == 0:\n",
    "            print \"Processed {}\".format(i)\n",
    "print \"Users: {}, Subreddits: {}\".format(len(users), len(subreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the mappings.\n",
    "with open(\"../bigData/pixie/indexToUserId\", 'w') as outfile:\n",
    "    for userId in users:\n",
    "        outfile.write(userId + \"\\n\")\n",
    "with open(\"../bigData/pixie/indexToSubredditId\", 'w') as outfile:\n",
    "    for subredditId in subreddits:\n",
    "        outfile.write(subredditId + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we generate a list of lists for each feature, numbered as follows:\n",
    "1. The index itself\n",
    "2. The comment count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the two mappings\n",
    "indexToUserId, userIdToIndex = tools.loadIndexToUserId(\"../bigData/pixie/indexToUserId\")\n",
    "indexToSubredditId, subredditIdToIndex = tools.loadIndexToUserId(\"../bigData/pixie/indexToSubredditId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Calculate total counts for each index.\n",
    "userIndexToTotalComments = [0] * len(indexToUserId)\n",
    "# subredditIndexToTotalComments = [0] * len(indexToSubredditId)\n",
    "with open(\"../bigData/finalGeneration/submissionsGraph\", 'r') as f:\n",
    "    for line in f:\n",
    "        userId, subredditId, commentCountStr = line.split()\n",
    "        if userId not in userIdToIndex or subredditId not in subredditIdToIndex:\n",
    "            continue\n",
    "        commentCount = float(commentCountStr)\n",
    "        \n",
    "        userIndex = userIdToIndex[userId]\n",
    "        subredditIndex = subredditIdToIndex[subredditId]\n",
    "        userIndexToTotalComments[userIndex] += np.sqrt(commentCount)\n",
    "        # subredditIndexToTotalComments[subredditIndex] += commentCount\n",
    "        \n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000000\n",
      "Users: 6283057, Subreddits: 152976\n",
      "Done Sorting\n"
     ]
    }
   ],
   "source": [
    "# Make the list of weights.\n",
    "userIndexToSubreddits = [[] for i in range(len(indexToUserId))]\n",
    "subredditIndexToUsers = [[] for i in range(len(indexToSubredditId))]\n",
    "with open(\"../bigData/finalGeneration/submissionsGraph\", 'r') as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        userId, subredditId, commentCountStr = line.split()\n",
    "        if userId not in userIdToIndex or subredditId not in subredditIdToIndex:\n",
    "            continue\n",
    "        commentCount = float(commentCountStr)\n",
    "        \n",
    "        userIndex = userIdToIndex[userId]\n",
    "        subredditIndex = subredditIdToIndex[subredditId]\n",
    "        userTotalComments = float(userIndexToTotalComments[userIndex])\n",
    "        \n",
    "        userIndexToSubreddits[userIndex].append((np.sqrt(commentCount) / userTotalComments, subredditIndex))\n",
    "        # subredditIndexToUsers[subredditIndex].append((commentCount, userIndex))\n",
    "        subredditIndexToUsers[subredditIndex].append(userIndex)\n",
    "        if i % 10000000 == 0:\n",
    "            print \"Processed {}\".format(i)\n",
    "print \"Users: {}, Subreddits: {}\".format(len(userIndexToSubreddits), len(subredditIndexToUsers))\n",
    "\n",
    "# Sort.\n",
    "for subreddits in userIndexToSubreddits:\n",
    "    subreddits.sort(reverse=True)\n",
    "\"\"\"\n",
    "for subredditIndex, users in enumerate(subredditIndexToUsers):\n",
    "    users.sort(reverse=True)\n",
    "    users[:] = users[:100]\n",
    "    subredditTotalComments = float(reduce(lambda total, nextPair: total + nextPair[0], users, 0))\n",
    "    for i in range(len(users)):\n",
    "        users[i] = (users[i][0] / subredditTotalComments, users[i][1])\n",
    "\"\"\"\n",
    "print \"Done Sorting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Output.\n",
    "with open(\"../bigData/pixie/userIndexToSubreddits\", 'w') as outfile:\n",
    "    for subreddits in userIndexToSubreddits:\n",
    "        line = \"\"\n",
    "        for weight, subredditIndex in subreddits:\n",
    "            line += \" {0:.9f} {1}\".format(weight, subredditIndex)\n",
    "        outfile.write(line + \"\\n\")\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open(\"../bigData/pixie/subredditIndexToUsers\", 'w') as outfile:\n",
    "    for users in subredditIndexToUsers:\n",
    "        line = \"\"\n",
    "        for weight, userIndex in users:\n",
    "            line += \" {0:.9f} {1}\".format(weight, userIndex)\n",
    "        outfile.write(line + \"\\n\")\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to weighted input graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's too slow if we have to walk through the entire list during the random walk, so we convert to a more efficient representation which will take more RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading user id indexes\n",
      "Loading subreddit id indexes\n",
      "Loading user id to old subreddits\n",
      "Loading user id to new subreddits\n",
      "Loading subreddit id to name\n"
     ]
    }
   ],
   "source": [
    "# Load everything.\n",
    "print \"Loading user id indexes\"\n",
    "indexToUserId, userIdToIndex = tools.loadIndexToUserId(\"../bigData/pixie/indexToUserId\")\n",
    "print \"Loading subreddit id indexes\"\n",
    "indexToSubredditId, subredditIdToIndex = tools.loadIndexToUserId(\"../bigData/pixie/indexToSubredditId\")\n",
    "print \"Loading user id to old subreddits\"\n",
    "userIdToOldSubreddits = tools.getUserIdToSubredditsByType(\"../bigData/finalGeneration/expUsers\", \"oldSubreddits\")\n",
    "print \"Loading user id to new subreddits\"\n",
    "userIdToNewSubreddits = tools.getUserIdToSubredditsByType(\"../bigData/finalGeneration/expUsers\", \"newSubreddits\")\n",
    "print \"Loading subreddit id to name\"\n",
    "subredditIdToName = tools.read_subreddit_names(\"../bigData/subredditIdToName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Loading user index to subreddits\"\n",
    "userIndexToSubreddits = None\n",
    "with open(\"../bigData/pixie/userIndexToSubreddits.pkl\", 'r') as infile:\n",
    "    userIndexToSubreddits = pickle.load(infile)\n",
    "print \"Loading subreddit index to user\"\n",
    "subredditIndexToUsers = None\n",
    "with open(\"../bigData/pixie/subredditIndexToUsers.pkl\", 'r') as infile:\n",
    "    subredditIndexToUsers = pickle.load(infile)\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 elizle\n",
      "Processing 1 spikespaz\n",
      "Processing 2 Woodpecker16669\n",
      "Processing 3 Dumpstertrash1\n",
      "Processing 4 Avalollk\n",
      "Processing 6 Zhangathan_Jon\n",
      "Processing 7 mooniesoloonie\n",
      "Processing 8 TorreTiger25\n",
      "Processing 10 thomasmagnum\n",
      "Processing 13 Deridex3101\n",
      "Processing 14 absolince\n",
      "Processing 15 farkhipov\n",
      "Processing 16 UnvaccinatedAutist\n",
      "Processing 17 pandillasexo\n",
      "Processing 18 show_me_the\n",
      "Processing 19 elbowe21\n",
      "Processing 20 BigAbbott\n",
      "Processing 21 AssasinButt\n",
      "Processing 24 Scissor_Runner12\n",
      "Processing 25 luck_panda\n",
      "Processing 26 coheedcollapse\n",
      "Processing 27 thomasd4nkengine\n",
      "Processing 28 BelievingEal21\n",
      "Processing 31 Brunsy89\n",
      "Processing 32 BrownMan97\n",
      "Processing 33 art_hoe1\n",
      "Processing 35 harpuajim25\n",
      "Processing 36 Pro_phet\n",
      "Processing 37 iamliterallysatan\n",
      "Processing 38 GTL5427\n",
      "Processing 39 FocusedADD\n",
      "Processing 40 habibiiiiiii\n",
      "Processing 41 Benjii117\n",
      "Processing 42 ZombieJesus1987\n",
      "Processing 44 msammy07\n",
      "Processing 45 Vag_Assasin\n",
      "Processing 46 Sockoram\n",
      "Processing 47 nerveless\n",
      "Processing 48 hurricane1197\n",
      "Processing 50 Miiiiinx\n",
      "Processing 51 KingOfNiFe\n",
      "Processing 52 aN1mosity_\n",
      "Processing 53 FoxClass\n",
      "Processing 54 The_Fun_Sized\n",
      "Processing 55 Pardnerr\n",
      "Processing 56 gueriLLaPunK\n",
      "Processing 57 DrainTheMuck\n",
      "Processing 59 LazyPrinciple\n",
      "Processing 60 Slayer_Blake\n",
      "Processing 61 TEKNOC\n",
      "Processing 62 MarvelRay\n",
      "Processing 63 HyruleHeroLink\n",
      "Processing 64 UnrequitedLovecraft\n",
      "Processing 65 TheCodGamer\n",
      "Processing 66 Sw0rDz\n",
      "Processing 67 lostintherandom\n",
      "Processing 68 ConfessorxXx\n",
      "Processing 69 TickleGrenade\n",
      "Processing 70 lakota1360\n",
      "Processing 71 Ghosthammer686\n",
      "Processing 72 Frozenicypole\n",
      "Processing 73 ElephantRattle\n",
      "Processing 74 Wazum\n",
      "Processing 75 carcar134134\n",
      "Processing 76 ghastlyactions\n",
      "Processing 77 kirtmew\n",
      "Processing 78 daishi-tech\n",
      "Processing 80 elnolog31\n",
      "Processing 81 throwaway9889616\n",
      "Processing 82 Ashybuttons\n",
      "Processing 83 potholes_and_euchre\n",
      "Processing 86 Damn1981\n",
      "Processing 87 colinmoore\n",
      "Processing 88 Joshdone\n",
      "Processing 89 N0tAG00dUserName\n",
      "Processing 91 YbarMaster27\n",
      "Processing 93 SouravMeh\n",
      "Processing 94 handsoapshimada\n",
      "Processing 96 Arcturus572\n",
      "Processing 97 TheBearKat\n",
      "Processing 98 Logicpolice9\n",
      "Processing 99 poppoppypop0\n",
      "N: 100000, alpha: 0.5, precision@10: 0.054, mrr: 0.153641487678, map: 0.0329193745047\n"
     ]
    }
   ],
   "source": [
    "# Reload module and set seeds.\n",
    "reload(pixie)\n",
    "random.seed(7224)\n",
    "np.random.seed(7224)\n",
    "\n",
    "# Parameters.\n",
    "N = 100000\n",
    "alpha = 0.5\n",
    "\n",
    "# Get recs.\n",
    "precision10 = 0\n",
    "mrrMetric = 0\n",
    "mapMetric = 0\n",
    "numUsers = 0\n",
    "for i, (userId, oldSubreddits) in enumerate(userIdToOldSubreddits.iteritems()):\n",
    "    if userId not in userIdToIndex:\n",
    "        continue\n",
    "    \n",
    "    print \"Processing {} {}\".format(i, userId)\n",
    "    oldSubreddits = {}\n",
    "    for weight, subredditIndex in userIndexToSubreddits[userIdToIndex[userId]]:\n",
    "        oldSubreddits[indexToSubredditId[subredditIndex]] = weight\n",
    "    recs = pixie.getRecs(oldSubreddits,\n",
    "                         subredditIdToIndex,\n",
    "                         indexToSubredditId,\n",
    "                         userIndexToSubreddits,\n",
    "                         subredditIndexToUsers,\n",
    "                         N, None, alpha)\n",
    "    \n",
    "    # Precision at 10.\n",
    "    goodRec10Count = 0\n",
    "    for _, rec in recs[:10]:\n",
    "        if rec in userIdToNewSubreddits[userId]:\n",
    "            goodRec10Count += 1\n",
    "    precision10 += goodRec10Count / 10.\n",
    "    \n",
    "    # Mean Reciprocal Rank.\n",
    "    for rank, (_, rec) in enumerate(recs, 1):\n",
    "        if rec in userIdToNewSubreddits[userId]:\n",
    "            mrrMetric += 1. / rank\n",
    "            break\n",
    "    \n",
    "    # Mean Average Precision.\n",
    "    correctSoFar = 0\n",
    "    numActual = float(len(userIdToNewSubreddits[userId]))\n",
    "    for rank, (_, rec) in enumerate(recs, 1):\n",
    "        if rec in userIdToNewSubreddits[userId]:\n",
    "            correctSoFar += 1\n",
    "            mapMetric += correctSoFar / numActual / rank\n",
    "\n",
    "totalUsers = float(len(userIdToOldSubreddits))\n",
    "print \"N: {}, alpha: {}, precision@10: {}, mrr: {}, map: {}\".format(\n",
    "    N,\n",
    "    alpha,\n",
    "    precision10 / totalUsers,\n",
    "    mrrMetric / totalUsers,\n",
    "    mapMetric / totalUsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5714285714285714, 12707), (0.2857142857142857, 29179), (0.14285714285714285, 53376)]\n",
      "truelose\n"
     ]
    }
   ],
   "source": [
    "print userIndexToSubreddits[1]\n",
    "print indexToUserId[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
