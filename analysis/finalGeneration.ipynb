{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tools' from 'tools.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import gc\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tools\n",
    "reload(tools)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process January to May Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the original graph data for Jan-May, with all the users, but without the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../bigData/userSubreddit/RC_2018-01\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processing ../bigData/userSubreddit/RC_2018-02\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processing ../bigData/userSubreddit/RC_2018-03\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processed 18000000\n",
      "Processing ../bigData/userSubreddit/RC_2018-04\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processed 18000000\n",
      "Processing ../bigData/userSubreddit/RC_2018-05\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processed 18000000\n",
      "Processed 19000000\n"
     ]
    }
   ],
   "source": [
    "subredditIdToName = tools.read_subreddit_names(\"../bigData/subredditIdToName\")\n",
    "infilenamePrefix = \"../bigData/userSubreddit/RC_2018-0\"\n",
    "userIdToOldSubreddits = collections.defaultdict(set)\n",
    "for i in range(1, 6):\n",
    "    infilename = infilenamePrefix + str(i)\n",
    "    tools.getUserIdToSubreddits(infilename, userIdToOldSubreddits, subredditIdToName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save a list of users with less than 3 subreddits in the months of Jan to May, and a list of subreddits with at least 10 users in the months of Jan to May."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../bigData/finalGeneration/userIdsToFilter', 'w') as outfile:\n",
    "    for userId, subreddits in userIdToOldSubreddits.iteritems():\n",
    "        if len(subreddits) < 3:\n",
    "            outfile.write(\"{}\\n\".format(userId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subredditUserCounts = collections.defaultdict(int)\n",
    "for userId, subreddits in userIdToOldSubreddits.iteritems():\n",
    "    for subreddit in subreddits:\n",
    "        subredditUserCounts[subreddit] += 1\n",
    "with open('../bigData/finalGeneration/subredditIdsToKeep', 'w') as outfile:\n",
    "    for subreddit, count in subredditUserCounts.iteritems():\n",
    "        if count >= 10:\n",
    "            outfile.write(\"{}\\n\".format(subreddit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reload the graph with the weights and filters and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../bigData/userSubreddit/RC_2018-01\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 6000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processing ../bigData/userSubreddit/RC_2018-02\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processing ../bigData/userSubreddit/RC_2018-03\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processed 18000000\n",
      "Processing ../bigData/userSubreddit/RC_2018-04\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processed 18000000\n",
      "Processing ../bigData/userSubreddit/RC_2018-05\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 17000000\n",
      "Processed 18000000\n",
      "Processed 19000000\n",
      "Total users: 4052716\n"
     ]
    }
   ],
   "source": [
    "userIdsToFilter = set()\n",
    "tools.readUserIdFilters(\"../bigData/finalGeneration/userIdsToFilter\", userIdsToFilter)\n",
    "subredditIdsToKeep = set()\n",
    "tools.readUserIdFilters(\"../bigData/finalGeneration/subredditIdsToKeep\", subredditIdsToKeep)\n",
    "\n",
    "infilenamePrefix = \"../bigData/userSubreddit/RC_2018-0\"\n",
    "userIdToOldSubreddits = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "for i in range(1, 6):\n",
    "    infilename = infilenamePrefix + str(i)\n",
    "    tools.getUserIdToSubreddits(infilename, userIdToOldSubreddits, subredditIdsToKeep, userIdsToFilter, True)\n",
    "print \"Total users: {}\".format(len(userIdToOldSubreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../bigData/finalGeneration/inputGraph', 'w') as outfile:\n",
    "    for userId, subreddits in userIdToOldSubreddits.iteritems():\n",
    "        for subreddit, count in subreddits.iteritems():\n",
    "            outfile.write(\"{} {} {}\\n\".format(userId, subreddit, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs:\n",
    "    - inputGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process June"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the June data, with the filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../bigData/userSubreddit/RC_2018-06\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processed 18000000\n",
      "Processed 19000000\n",
      "Number of users: 3649104\n"
     ]
    }
   ],
   "source": [
    "infilename = \"../bigData/userSubreddit/RC_2018-06\"\n",
    "userIdToNewSubreddits = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "tools.getUserIdToSubreddits(infilename, userIdToNewSubreddits, subredditIdsToKeep, userIdsToFilter, True)\n",
    "print \"Number of users: {}\".format(len(userIdToNewSubreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Builds actualNewSubreddits, answer key for recommendations, in JSON format\n",
    "# {userId: <string>, newSubreddits: {<stringSubreddit>: <intCount>, ...}}\n",
    "with open('../bigData/finalGeneration/actualNewSubreddits', 'w') as outfile:\n",
    "    for userId, newSubreddits in userIdToNewSubreddits.iteritems():\n",
    "        if userId not in userIdToOldSubreddits:\n",
    "            continue\n",
    "            \n",
    "        userJson = {\"userId\": userId, \"newSubreddits\": {}}\n",
    "        for subredditId, count in newSubreddits.iteritems():\n",
    "            if subredditId not in userIdToOldSubreddits[userId]:\n",
    "                userJson[\"newSubreddits\"][subredditId] = count\n",
    "        outfile.write(json.dumps(userJson) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs:\n",
    "    - actualNewSubreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dev and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../bigData/finalGeneration/inputGraph\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processed 18000000\n",
      "Processed 19000000\n",
      "Processed 20000000\n",
      "Processed 21000000\n",
      "Processed 22000000\n",
      "Processed 23000000\n",
      "Processed 24000000\n",
      "Processed 25000000\n",
      "Processed 26000000\n",
      "Processed 27000000\n",
      "Processed 28000000\n",
      "Processed 29000000\n",
      "Processed 30000000\n",
      "Processed 31000000\n",
      "Processed 32000000\n",
      "Processed 33000000\n",
      "Processed 34000000\n",
      "Processed 35000000\n",
      "Processed 36000000\n",
      "Processed 37000000\n",
      "Processed 38000000\n",
      "Processed 39000000\n",
      "Processed 40000000\n",
      "Processed 41000000\n",
      "Processed 42000000\n",
      "Processed 43000000\n",
      "Processed 44000000\n",
      "Processed 45000000\n",
      "Processed 46000000\n",
      "Processed 47000000\n",
      "Processed 48000000\n",
      "Processed 49000000\n",
      "Processed 50000000\n",
      "Processed 51000000\n",
      "Processed 52000000\n",
      "Number of users: 4052716\n"
     ]
    }
   ],
   "source": [
    "# Load inputGraph, include weights.\n",
    "userIdToOldSubreddits = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "tools.getUserIdToSubreddits(\"../bigData/finalGeneration/inputGraph\", userIdToOldSubreddits, includeCounts=True)\n",
    "print \"Number of users: {}\".format(len(userIdToOldSubreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Number of users: 2812982\n"
     ]
    }
   ],
   "source": [
    "# Load actualNewSubreddits, include weights.\n",
    "actualNewSubreddits = tools.getUserIdToSubredditsByType(\"../bigData/finalGeneration/actualNewSubreddits\", \"newSubreddits\")\n",
    "print \"Number of users: {}\".format(len(actualNewSubreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidate users: 114314\n"
     ]
    }
   ],
   "source": [
    "# Output some random subset for dev and test.\n",
    "# Format is {userId: <string>, oldSubreddits: {<stringSubreddit>: <intCount>, ...}, newSubreddits: {<stringSubreddit>: <intCount>, ...}}\n",
    "random.seed(7224)\n",
    "\n",
    "candidateUserIds = []\n",
    "for userId, subreddits in actualNewSubreddits.iteritems():\n",
    "    if len(subreddits) >= 10 and len(subreddits) <= 100:\n",
    "        candidateUserIds.append(userId)\n",
    "random.shuffle(candidateUserIds)        \n",
    "print \"Number of candidate users: {}\".format(len(candidateUserIds))\n",
    "\n",
    "with open('../bigData/finalGeneration/devUsers', 'w') as devfile, \\\n",
    "     open('../bigData/finalGeneration/testUsers', 'w') as testfile:\n",
    "    counter = 0\n",
    "    for userId in candidateUserIds:\n",
    "        userJson = {\"userId\": userId, \n",
    "                    \"newSubreddits\": actualNewSubreddits[userId],\n",
    "                    \"oldSubreddits\": userIdToOldSubreddits[userId]}\n",
    "        if counter < 100:\n",
    "            devfile.write(json.dumps(userJson) + \"\\n\")\n",
    "        elif counter < 200:\n",
    "            testfile.write(json.dumps(userJson) + \"\\n\")\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
