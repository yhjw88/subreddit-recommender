{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a continuation of userSubredditAnalysis. There are two inputs:\n",
    "\n",
    "1. \"inputGraph\", which is a mapping of userId to the subreddits the user commented on, weighted by counts, from January to May, with all 1-subreddit users filtered out.\n",
    "2. \"actualNewSubreddits\", which is a JSON list of `{userId: <string>, newSubreddits: {<stringSubreddit>: <intCount>, ...}`, from June.\n",
    "\n",
    "The output is a randomized devSet and a randomized testSet. The preliminary analysis and rationale behind the generated sets can be found in userSubredditAnalysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tools' from 'tools.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import gc\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tools\n",
    "reload(tools)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reformatting (please skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of actualNewSubreddits is inconsistent, this reformats it to be consistent. This need only be run once in a lifetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n"
     ]
    }
   ],
   "source": [
    "# Load and save actualNewSubreddits in a better format.\n",
    "with open(\"../bigData/analysis/actualNewSubreddits\", 'r') as infile, \\\n",
    "     open(\"../bigData/analysis/actualNewSubreddits2\", 'w') as outfile:\n",
    "    for i, line in enumerate(infile, 1):\n",
    "        lineJson = json.loads(line)\n",
    "        newSubreddits = {}\n",
    "        for thing in lineJson[\"newSubreddits\"]:\n",
    "            subreddit = thing.keys()[0]\n",
    "            newSubreddits[subreddit] = thing[subreddit]\n",
    "        lineJson[\"newSubreddits\"] = newSubreddits\n",
    "        outfile.write(json.dumps(lineJson) + \"\\n\")\n",
    "        if i % 1000000 == 0:\n",
    "            print \"Processed {}\".format(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../bigData/analysis/inputGraph\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processed 18000000\n",
      "Processed 19000000\n",
      "Processed 20000000\n",
      "Processed 21000000\n",
      "Processed 22000000\n",
      "Processed 23000000\n",
      "Processed 24000000\n",
      "Processed 25000000\n",
      "Processed 26000000\n",
      "Processed 27000000\n",
      "Processed 28000000\n",
      "Processed 29000000\n",
      "Processed 30000000\n",
      "Processed 31000000\n",
      "Processed 32000000\n",
      "Processed 33000000\n",
      "Processed 34000000\n",
      "Processed 35000000\n",
      "Processed 36000000\n",
      "Processed 37000000\n",
      "Processed 38000000\n",
      "Processed 39000000\n",
      "Processed 40000000\n",
      "Processed 41000000\n",
      "Processed 42000000\n",
      "Processed 43000000\n",
      "Processed 44000000\n",
      "Processed 45000000\n",
      "Processed 46000000\n",
      "Processed 47000000\n",
      "Processed 48000000\n",
      "Processed 49000000\n",
      "Processed 50000000\n",
      "Processed 51000000\n",
      "Processed 52000000\n",
      "Processed 53000000\n",
      "Processed 54000000\n",
      "Processed 55000000\n",
      "Number of users: 5374426\n"
     ]
    }
   ],
   "source": [
    "# Load inputGraph, include weights.\n",
    "userIdToOldSubreddits = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "tools.getUserIdToSubreddits(\"../bigData/analysis/inputGraph\", userIdToOldSubreddits, includeCounts=True)\n",
    "print \"Number of users: {}\".format(len(userIdToOldSubreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Number of users: 3258157\n"
     ]
    }
   ],
   "source": [
    "# Load actualNewSubreddits, include weights.\n",
    "actualNewSubreddits = tools.getUserIdToSubredditsByType(\"../bigData/analysis/actualNewSubreddits\", \"newSubreddits\")\n",
    "print \"Number of users: {}\".format(len(actualNewSubreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidate users: 118620\n"
     ]
    }
   ],
   "source": [
    "# Output some random subset for dev and test.\n",
    "# Format is {userId: <string>, oldSubreddits: {<stringSubreddit>: <intCount>, ...}, newSubreddits: {<stringSubreddit>: <intCount>, ...}}\n",
    "random.seed(7224)\n",
    "\n",
    "candidateUserIds = []\n",
    "for userId, subreddits in actualNewSubreddits.iteritems():\n",
    "    if len(subreddits) >= 10 and len(subreddits) <= 100:\n",
    "        candidateUserIds.append(userId)\n",
    "random.shuffle(candidateUserIds)        \n",
    "print \"Number of candidate users: {}\".format(len(candidateUserIds))\n",
    "\n",
    "with open('../bigData/devTest/devUsers', 'w') as devfile, \\\n",
    "     open('../bigData/devTest/testUsers', 'w') as testfile:\n",
    "    counter = 0\n",
    "    for userId in candidateUserIds:\n",
    "        userJson = {\"userId\": userId, \n",
    "                    \"newSubreddits\": actualNewSubreddits[userId],\n",
    "                    \"oldSubreddits\": userIdToOldSubreddits[userId]}\n",
    "        if counter < 100:\n",
    "            devfile.write(json.dumps(userJson) + \"\\n\")\n",
    "        elif counter < 200:\n",
    "            testfile.write(json.dumps(userJson) + \"\\n\")\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
