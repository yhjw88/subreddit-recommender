{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tools' from 'tools.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import cPickle as pickle\n",
    "import gc\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tools\n",
    "reload(tools)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../bigData/finalGeneration/inputGraph\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processed 18000000\n",
      "Processed 19000000\n",
      "Processed 20000000\n",
      "Processed 21000000\n",
      "Processed 22000000\n",
      "Processed 23000000\n",
      "Processed 24000000\n",
      "Processed 25000000\n",
      "Processed 26000000\n",
      "Processed 27000000\n",
      "Processed 28000000\n",
      "Processed 29000000\n",
      "Processed 30000000\n",
      "Processed 31000000\n",
      "Processed 32000000\n",
      "Processed 33000000\n",
      "Processed 34000000\n",
      "Processed 35000000\n",
      "Processed 36000000\n",
      "Processed 37000000\n",
      "Processed 38000000\n",
      "Processed 39000000\n",
      "Processed 40000000\n",
      "Processed 41000000\n",
      "Processed 42000000\n",
      "Processed 43000000\n",
      "Processed 44000000\n",
      "Processed 45000000\n",
      "Processed 46000000\n",
      "Processed 47000000\n",
      "Processed 48000000\n",
      "Processed 49000000\n",
      "Processed 50000000\n",
      "Processed 51000000\n",
      "Processed 52000000\n",
      "Number of users: 4052716\n"
     ]
    }
   ],
   "source": [
    "# Load inputGraph, include weights.\n",
    "userIdToOldSubreddits = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "tools.getUserIdToSubreddits(\"../bigData/finalGeneration/inputGraph\", userIdToOldSubreddits, includeCounts=True)\n",
    "print \"Number of users: {}\".format(len(userIdToOldSubreddits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing is specific to collabFilter. The current input format is inefficient, so we transform the input a little. Note that this is the similar to the \"Output to prep for collab filtering\" section of userSubredditAnalysis, except streamlined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output indexToUserId, intended to be indexed 0 to len(indexToUserIds) - 1\n",
    "with open(\"../bigData/collabFilter/indexToUserId\", 'w') as outfile:\n",
    "    for userId, _ in userIdToOldSubreddits.iteritems():\n",
    "        outfile.write(userId + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Generating\n",
      "Done Outputting\n"
     ]
    }
   ],
   "source": [
    "# Output subredditVectors, each subreddit represented as a sparse vector of userIndex -> count\n",
    "# Format is json: {<subredditId>: {<userIndex>:<count>, <userIndex>: <count> ...}}\n",
    "indexToUserId, userIdToIndex = tools.loadIndexToUserId(\"../bigData/collabFilter/indexToUserId\")\n",
    "subredditVectors = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "for userId, subreddits in userIdToOldSubreddits.iteritems():\n",
    "    for subreddit, count in subreddits.iteritems():\n",
    "        subredditVectors[subreddit][userIdToIndex[userId]] += count\n",
    "print \"Done Generating\"\n",
    "\n",
    "with open(\"../bigData/collabFilter/subredditVectors.json\", 'w') as outfile:\n",
    "    for subreddit, subredditVector in subredditVectors.iteritems():\n",
    "        subredditJson = {}\n",
    "        subredditJson[subreddit] = subredditVector\n",
    "        outfile.write(json.dumps(subredditJson) + \"\\n\")\n",
    "print \"Done Outputting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Json format is too slow, instead pickle it.\n",
    "subredditVectors = tools.loadSubredditVectors(\"../bigData/collabFilter/subredditVectors.json\")\n",
    "with open(\"../bigData/collabFilter/subredditVectors.pkl\", 'w') as outfile:\n",
    "    pickle.dump(subredditVectors, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subredditVectors = tools.loadSubredditVectors(\"../bigData/collabFilter/subredditVectors.pkl\")\n",
    "with open(\"../bigData/collabFilter/cosineSims.pkl\", 'r') as infile:\n",
    "    similarSubredditDict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30989903258\n",
      "0.0981113696629\n"
     ]
    }
   ],
   "source": [
    "# print subredditVectors[\"t5_2xxyj\"]\n",
    "# print similarSubredditDict[\"t5_2xxyj\"][:10]\n",
    "print tools.cosineSim(\"t5_2xxyj\", \"t5_2w708\", subredditVectors, {})\n",
    "print tools.jaccardSim(subredditVectors[\"t5_2xxyj\"], subredditVectors[\"t5_2w708\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
