{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tools' from 'tools.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import cPickle as pickle\n",
    "import itertools\n",
    "import gc\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import snap\n",
    "import tools\n",
    "reload(tools)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use SNAP and see how much space it takes. First the node encodings.\n",
    "\n",
    "- indexToUserId: Maps indexes from 0 to (# users - 1) to the userId\n",
    "- indexToSubreddit: Maps indexes from 0 to (# subreddits - 1) to the subredditId\n",
    "- THRESHOLD: All users will have index < THRESHOLD, all subreddits will be stored as THRESHOLD + index. The THRESHOLD will be 5,000,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../bigData/finalGeneration/inputGraph\n",
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processed 18000000\n",
      "Processed 19000000\n",
      "Processed 20000000\n",
      "Processed 21000000\n",
      "Processed 22000000\n",
      "Processed 23000000\n",
      "Processed 24000000\n",
      "Processed 25000000\n",
      "Processed 26000000\n",
      "Processed 27000000\n",
      "Processed 28000000\n",
      "Processed 29000000\n",
      "Processed 30000000\n",
      "Processed 31000000\n",
      "Processed 32000000\n",
      "Processed 33000000\n",
      "Processed 34000000\n",
      "Processed 35000000\n",
      "Processed 36000000\n",
      "Processed 37000000\n",
      "Processed 38000000\n",
      "Processed 39000000\n",
      "Processed 40000000\n",
      "Processed 41000000\n",
      "Processed 42000000\n",
      "Processed 43000000\n",
      "Processed 44000000\n",
      "Processed 45000000\n",
      "Processed 46000000\n",
      "Processed 47000000\n",
      "Processed 48000000\n",
      "Processed 49000000\n",
      "Processed 50000000\n",
      "Processed 51000000\n",
      "Processed 52000000\n",
      "Number of users: 4052716\n"
     ]
    }
   ],
   "source": [
    "# Load inputGraph, include weights.\n",
    "inputGraph = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "tools.getUserIdToSubreddits(\"../bigData/finalGeneration/inputGraph\", inputGraph, includeCounts=True)\n",
    "print \"Number of users: {}\".format(len(inputGraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output indexToUserId, intended to be indexed 0 to len(indexToUserIds) - 1\n",
    "with open(\"../bigData/pixie/indexToUserId\", 'w') as outfile:\n",
    "    for userId, _ in userIdToOldSubreddits.iteritems():\n",
    "        outfile.write(userId + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output indexToSubredditId, intended to be indexed 0 to len(indexToSubredditId) - 1\n",
    "subredditIdSet = set()\n",
    "for userId, subreddits in userIdToOldSubreddits.iteritems():\n",
    "    for subreddit in subreddits:\n",
    "        subredditIdSet.add(subreddit)\n",
    "\n",
    "with open(\"../bigData/pixie/indexToSubredditId\", 'w') as outfile:\n",
    "    for subredditId in subredditIdSet:\n",
    "        outfile.write(subredditId + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the graph into SNAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c0af6e2c9d7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0muserId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubredditId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommentCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0minputGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddEdge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserIdToIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserId\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTHRESHOLD\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubredditIdToIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubredditId\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the two mappings.\n",
    "THRESHOLD = 5000000\n",
    "indexToUserId, userIdToIndex = tools.loadIndexToUserId(\"../bigData/pixie/indexToUserId\")\n",
    "indexToSubredditId, subredditIdToIndex = tools.loadIndexToUserId(\"../bigData/pixie/indexToSubredditId\")\n",
    "\n",
    "inputGraph = snap.TUNGraph.New()\n",
    "for i in range(len(indexToUserId)):\n",
    "    inputGraph.AddNode(i)\n",
    "for i in range(len(indexToSubredditId)):\n",
    "    inputGraph.AddNode(THRESHOLD + i)\n",
    "with open(\"../bigData/finalGeneration/inputGraph\", 'r') as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        userId, subredditId, commentCount = line.split()\n",
    "        inputGraph.AddEdge(userIdToIndex[userId], THRESHOLD + subredditIdToIndex[subredditId])\n",
    "\n",
    "        if i % 1000000 == 0:\n",
    "            print \"Processed {}\".format(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNAP too slow, will instead generate two lists of adjacency lists.\n",
    "- userIndexToSubreddits\n",
    "- subredditIndexToUsers\n",
    "\n",
    "This also means we will no longer be using THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000\n",
      "Processed 2000000\n",
      "Processed 3000000\n",
      "Processed 4000000\n",
      "Processed 5000000\n",
      "Processed 6000000\n",
      "Processed 7000000\n",
      "Processed 8000000\n",
      "Processed 9000000\n",
      "Processed 10000000\n",
      "Processed 11000000\n",
      "Processed 12000000\n",
      "Processed 13000000\n",
      "Processed 14000000\n",
      "Processed 15000000\n",
      "Processed 16000000\n",
      "Processed 17000000\n",
      "Processed 18000000\n",
      "Processed 19000000\n",
      "Processed 20000000\n",
      "Processed 21000000\n",
      "Processed 22000000\n",
      "Processed 23000000\n",
      "Processed 24000000\n",
      "Processed 25000000\n",
      "Processed 26000000\n",
      "Processed 27000000\n",
      "Processed 28000000\n",
      "Processed 29000000\n",
      "Processed 30000000\n",
      "Processed 31000000\n",
      "Processed 32000000\n",
      "Processed 33000000\n",
      "Processed 34000000\n",
      "Processed 35000000\n",
      "Processed 36000000\n",
      "Processed 37000000\n",
      "Processed 38000000\n",
      "Processed 39000000\n",
      "Processed 40000000\n",
      "Processed 41000000\n",
      "Processed 42000000\n",
      "Processed 43000000\n",
      "Processed 44000000\n",
      "Processed 45000000\n",
      "Processed 46000000\n",
      "Processed 47000000\n",
      "Processed 48000000\n",
      "Processed 49000000\n",
      "Processed 50000000\n",
      "Processed 51000000\n",
      "Processed 52000000\n",
      "Users: 4052716, Subreddits: 54204\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load the two mappings.\n",
    "indexToUserId, userIdToIndex = tools.loadIndexToUserId(\"../bigData/pixie/indexToUserId\")\n",
    "indexToSubredditId, subredditIdToIndex = tools.loadIndexToUserId(\"../bigData/pixie/indexToSubredditId\")\n",
    "\n",
    "# Throw into lists.\n",
    "userIndexToSubreddits1 = [[] for i in range(len(indexToUserId))]\n",
    "subredditIndexToUsers1 = [[] for i in range(len(indexToSubredditId))]\n",
    "userIndexToSubreddits2 = [[] for i in range(len(indexToUserId))]\n",
    "subredditIndexToUsers2 = [[] for i in range(len(indexToSubredditId))]\n",
    "with open(\"../bigData/finalGeneration/inputGraph\", 'r') as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        userId, subredditId, commentCountStr = line.split()\n",
    "        userIndex = userIdToIndex[userId]\n",
    "        subredditIndex = subredditIdToIndex[subredditId]\n",
    "        commentCount = int(commentCountStr)\n",
    "        \n",
    "        userIndexToSubreddits1[userIndex].append(subredditIndex)\n",
    "        subredditIndexToUsers1[subredditIndex].append(userIndex)\n",
    "        userIndexToSubreddits2[userIndex].append(commentCount)\n",
    "        subredditIndexToUsers2[subredditIndex].append(commentCount)\n",
    "        if i % 1000000 == 0:\n",
    "            print \"Processed {}\".format(i)\n",
    "print \"Users: {}, Subreddits: {}\".format(len(userIndexToSubreddits1), len(subredditIndexToUsers1))\n",
    "            \n",
    "# Output.\n",
    "with open(\"../bigData/pixie/userIndexToSubreddits1.pkl\", 'w') as outfile:\n",
    "    pickle.dump(userIndexToSubreddits1, outfile)\n",
    "with open(\"../bigData/pixie/subredditIndexToUsers1.pkl\", 'w') as outfile:\n",
    "    pickle.dump(subredditIndexToUsers1, outfile)\n",
    "with open(\"../bigData/pixie/userIndexToSubreddits2.pkl\", 'w') as outfile:\n",
    "    pickle.dump(userIndexToSubreddits2, outfile)\n",
    "with open(\"../bigData/pixie/subredditIndexToUsers2.pkl\", 'w') as outfile:\n",
    "    pickle.dump(subredditIndexToUsers2, outfile)\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixie works very poorly without biasing the random walk. We will now weigh the random walks by making a list of 100 per user, and 10,000 per subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subreddit index to user\n",
      "Loading subreddit index to user\n"
     ]
    }
   ],
   "source": [
    "userIndexToSubreddits1 = None\n",
    "with open(\"../bigData/pixie/userIndexToSubreddits1.pkl\", 'r') as infile:\n",
    "    userIndexToSubreddits1 = pickle.load(infile)\n",
    "subredditIndexToUsers1 = None\n",
    "with open(\"../bigData/pixie/subredditIndexToUsers1.pkl\", 'r') as infile:\n",
    "    subredditIndexToUsers1 = pickle.load(infile)\n",
    "userIndexToSubreddits2 = None\n",
    "with open(\"../bigData/pixie/userIndexToSubreddits2.pkl\", 'r') as infile:\n",
    "    userIndexToSubreddits2 = pickle.load(infile)\n",
    "subredditIndexToUsers2 = None\n",
    "with open(\"../bigData/pixie/subredditIndexToUsers2.pkl\", 'r') as infile:\n",
    "    subredditIndexToUsers2 = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USER_LIST_SIZE = 100\n",
    "SUBREDDIT_LIST_SIZE = 10000\n",
    "# indexToUserId, userIdToIndex = tools.loadIndexToUserId(\"../bigData/pixie/indexToUserId\")\n",
    "# indexToSubredditId, subredditIdToIndex = tools.loadIndexToUserId(\"../bigData/pixie/indexToSubredditId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 999999\n",
      "User: 1999999\n",
      "User: 2999999\n",
      "User: 3999999\n",
      "Users: 4052716, avgLength: 99.6149278657\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# User -> Subreddits.\n",
    "userIndexToSubreddits = [[] for i in range(len(userIndexToSubreddits1))]\n",
    "for userIndex, subredditIndexes in enumerate(userIndexToSubreddits1):\n",
    "    totalComments = float(sum(userIndexToSubreddits2[userIndex]))\n",
    "    for subredditIndex, commentCount in zip(subredditIndexes, userIndexToSubreddits2[userIndex]):\n",
    "        numToAppend = int(np.round(commentCount / totalComments * USER_LIST_SIZE))\n",
    "        for i in range(numToAppend):\n",
    "            userIndexToSubreddits[userIndex].append(subredditIndex)\n",
    "    if (userIndex + 1) % 1000000 == 0:\n",
    "        print \"User: {}\".format(userIndex + 1)\n",
    "sumLengths = reduce(lambda total, nextList: total + len(nextList), userIndexToSubreddits, 0)\n",
    "print \"Users: {}, avgLength: {}\".format(len(userIndexToSubreddits), sumLengths / float(len(userIndexToSubreddits)))\n",
    "\n",
    "# Dump.\n",
    "with open(\"../bigData/pixie/userIndexToSubreddits.pkl\", 'w') as outfile:\n",
    "    pickle.dump(userIndexToSubreddits, outfile)\n",
    "print \"Done\"\n",
    "userIndexToSubreddits = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddit: 10000\n",
      "Subreddit: 20000\n",
      "Subreddit: 30000\n",
      "Subreddit: 40000\n",
      "Subreddit: 50000\n",
      "Subreddits: 54204, avgLength: 9947.33427422\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Subreddits -> User.\n",
    "subredditIndexToUsers = [[] for i in range(len(subredditIndexToUsers1))]\n",
    "for subredditIndex, userIndexes in enumerate(subredditIndexToUsers1):\n",
    "    totalComments = float(sum(subredditIndexToUsers2[subredditIndex]))\n",
    "    for userIndex, commentCount in zip(userIndexes, subredditIndexToUsers2[subredditIndex]):\n",
    "        numToAppend = int(np.round(commentCount / totalComments * SUBREDDIT_LIST_SIZE))\n",
    "        for i in range(numToAppend):\n",
    "            subredditIndexToUsers[subredditIndex].append(userIndex)\n",
    "    if (subredditIndex + 1) % 10000 == 0:\n",
    "        print \"Subreddit: {}\".format(subredditIndex + 1)\n",
    "sumLengths = reduce(lambda total, nextList: total + len(nextList), subredditIndexToUsers, 0)\n",
    "print \"Subreddits: {}, avgLength: {}\".format(len(subredditIndexToUsers), sumLengths / float(len(subredditIndexToUsers)))\n",
    "\n",
    "# Dump.\n",
    "with open(\"../bigData/pixie/subredditIndexToUsers.pkl\", 'w') as outfile:\n",
    "    pickle.dump(subredditIndexToUsers, outfile)\n",
    "print \"Done\"\n",
    "subredditIndexToUsers = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Pixie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really just for testing it, since we'd like to keep the whole graph in memory for a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pixie\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading user id indexes\n",
      "Loading subreddit id indexes\n",
      "Loading user id to old subreddits\n",
      "Loading user id to new subreddits\n",
      "Loading subreddit id to name\n",
      "Loading user index to subreddits\n"
     ]
    }
   ],
   "source": [
    "# Load everything.\n",
    "print \"Loading user id indexes\"\n",
    "indexToUserId, userIdToIndex = tools.loadIndexToUserId(\"../bigData/pixie/indexToUserId\")\n",
    "print \"Loading subreddit id indexes\"\n",
    "indexToSubredditId, subredditIdToIndex = tools.loadIndexToUserId(\"../bigData/pixie/indexToSubredditId\")\n",
    "print \"Loading user id to old subreddits\"\n",
    "userIdToOldSubreddits = tools.getUserIdToSubredditsByType(\"../bigData/finalGeneration/expUsers\", \"oldSubreddits\")\n",
    "print \"Loading user id to new subreddits\"\n",
    "userIdToNewSubreddits = tools.getUserIdToSubredditsByType(\"../bigData/finalGeneration/expUsers\", \"newSubreddits\")\n",
    "print \"Loading subreddit id to name\"\n",
    "subredditIdToName = tools.read_subreddit_names(\"../bigData/subredditIdToName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Loading user index to subreddits\"\n",
    "userIndexToSubreddits = None\n",
    "with open(\"../bigData/pixie/userIndexToSubreddits.pkl\", 'r') as infile:\n",
    "    userIndexToSubreddits = pickle.load(infile)\n",
    "print \"Loading subreddit index to user\"\n",
    "subredditIndexToUsers = None\n",
    "with open(\"../bigData/pixie/subredditIndexToUsers.pkl\", 'r') as infile:\n",
    "    subredditIndexToUsers = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 elizle\n",
      "Processing 1 spikespaz\n",
      "Processing 2 Woodpecker16669\n",
      "Processing 3 Dumpstertrash1\n",
      "Processing 4 Avalollk\n",
      "Processing 5 slickslink\n",
      "Processing 6 Zhangathan_Jon\n",
      "Processing 7 mooniesoloonie\n",
      "Processing 8 TorreTiger25\n",
      "Processing 9 Deenyc43\n",
      "Processing 10 thomasmagnum\n",
      "Processing 11 LangourDaydreams\n",
      "Processing 12 ChaosOnion\n",
      "Processing 13 Deridex3101\n",
      "Processing 14 absolince\n",
      "Processing 15 farkhipov\n",
      "Processing 16 UnvaccinatedAutist\n",
      "Processing 17 pandillasexo\n",
      "Processing 18 show_me_the\n",
      "Processing 19 elbowe21\n",
      "Processing 20 BigAbbott\n",
      "Processing 21 AssasinButt\n",
      "Processing 22 Danosaur42089\n",
      "Processing 23 amskeez\n",
      "Processing 24 Scissor_Runner12\n",
      "Processing 25 luck_panda\n",
      "Processing 26 coheedcollapse\n",
      "Processing 27 thomasd4nkengine\n",
      "Processing 28 BelievingEal21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixie.py:86: RuntimeWarning: divide by zero encountered in log\n",
      "  scalingFactors.append(oldSubredditDegree * (maxSubredditDegree - np.log(oldSubredditDegree)))\n",
      "pixie.py:86: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  scalingFactors.append(oldSubredditDegree * (maxSubredditDegree - np.log(oldSubredditDegree)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 29 bongtokent\n",
      "Processing 30 wanderershe-ra\n",
      "Processing 31 Brunsy89\n",
      "Processing 32 BrownMan97\n",
      "Processing 33 art_hoe1\n",
      "Processing 34 FF_Firefighter\n",
      "Processing 35 harpuajim25\n",
      "Processing 36 Pro_phet\n",
      "Processing 37 iamliterallysatan\n",
      "Processing 38 GTL5427\n",
      "Processing 39 FocusedADD\n",
      "Processing 40 habibiiiiiii\n",
      "Processing 41 Benjii117\n",
      "Processing 42 ZombieJesus1987\n",
      "Processing 43 PussyNoodle\n",
      "Processing 44 msammy07\n",
      "Processing 45 Vag_Assasin\n",
      "Processing 46 Sockoram\n",
      "Processing 47 nerveless\n",
      "Processing 48 hurricane1197\n",
      "Processing 49 jodiparks\n",
      "Processing 50 Miiiiinx\n",
      "Processing 51 KingOfNiFe\n",
      "Processing 52 aN1mosity_\n",
      "Processing 53 FoxClass\n",
      "Processing 54 The_Fun_Sized\n",
      "Processing 55 Pardnerr\n",
      "Processing 56 gueriLLaPunK\n",
      "Processing 57 DrainTheMuck\n",
      "Processing 58 archivalerie\n",
      "Processing 59 LazyPrinciple\n",
      "Processing 60 Slayer_Blake\n",
      "Processing 61 TEKNOC\n",
      "Processing 62 MarvelRay\n",
      "Processing 63 HyruleHeroLink\n",
      "Processing 64 UnrequitedLovecraft\n",
      "Processing 65 TheCodGamer\n",
      "Processing 66 Sw0rDz\n",
      "Processing 67 lostintherandom\n",
      "Processing 68 ConfessorxXx\n",
      "Processing 69 TickleGrenade\n",
      "Processing 70 lakota1360\n",
      "Processing 71 Ghosthammer686\n",
      "Processing 72 Frozenicypole\n",
      "Processing 73 ElephantRattle\n",
      "Processing 74 Wazum\n",
      "Processing 75 carcar134134\n",
      "Processing 76 ghastlyactions\n",
      "Processing 77 kirtmew\n",
      "Processing 78 daishi-tech\n",
      "Processing 79 2krazy4me\n",
      "Processing 80 elnolog31\n",
      "Processing 81 throwaway9889616\n",
      "Processing 82 Ashybuttons\n",
      "Processing 83 potholes_and_euchre\n",
      "Processing 84 LikeMyEighthUsername\n",
      "Processing 85 135redtoblue\n",
      "Processing 86 Damn1981\n",
      "Processing 87 colinmoore\n",
      "Processing 88 Joshdone\n",
      "Processing 89 N0tAG00dUserName\n",
      "Processing 90 AllshallloveTheQueen\n",
      "Processing 91 YbarMaster27\n",
      "Processing 92 rTidde77\n",
      "Processing 93 SouravMeh\n",
      "Processing 94 handsoapshimada\n",
      "Processing 95 drwzrd\n",
      "Processing 96 Arcturus572\n",
      "Processing 97 TheBearKat\n",
      "Processing 98 Logicpolice9\n",
      "Processing 99 poppoppypop0\n",
      "N: 100000, alpha: 0.5, precision@10: 0.142, mrr: 0.374839280018, map: 0.091791888459\n"
     ]
    }
   ],
   "source": [
    "# Reload module and set seeds.\n",
    "reload(pixie)\n",
    "random.seed(7224)\n",
    "np.random.seed(7224)\n",
    "\n",
    "# Parameters.\n",
    "N = 100000\n",
    "alpha = 0.5\n",
    "\n",
    "# Get recs.\n",
    "precision10 = 0\n",
    "mrrMetric = 0\n",
    "mapMetric = 0\n",
    "numUsers = 0\n",
    "for i, (userId, oldSubreddits) in enumerate(userIdToOldSubreddits.iteritems()):    \n",
    "    print \"Processing {} {}\".format(i, userId)\n",
    "    recs = pixie.getRecs(oldSubreddits,\n",
    "                         subredditIdToIndex,\n",
    "                         indexToSubredditId,\n",
    "                         userIndexToSubreddits,\n",
    "                         subredditIndexToUsers,\n",
    "                         N, None, alpha)\n",
    "    \n",
    "    # Precision at 10.\n",
    "    goodRec10Count = 0\n",
    "    for _, rec in recs[:10]:\n",
    "        if rec in userIdToNewSubreddits[userId]:\n",
    "            goodRec10Count += 1\n",
    "    precision10 += goodRec10Count / 10.\n",
    "    \n",
    "    # Mean Reciprocal Rank.\n",
    "    for rank, (_, rec) in enumerate(recs, 1):\n",
    "        if rec in userIdToNewSubreddits[userId]:\n",
    "            mrrMetric += 1. / rank\n",
    "            break\n",
    "    \n",
    "    # Mean Average Precision.\n",
    "    correctSoFar = 0\n",
    "    numActual = float(len(userIdToNewSubreddits[userId]))\n",
    "    for rank, (_, rec) in enumerate(recs, 1):\n",
    "        if rec in userIdToNewSubreddits[userId]:\n",
    "            correctSoFar += 1\n",
    "            mapMetric += correctSoFar / numActual / rank\n",
    "\n",
    "totalUsers = float(len(userIdToOldSubreddits))\n",
    "print \"N: {}, alpha: {}, precision@10: {}, mrr: {}, map: {}\".format(\n",
    "    N,\n",
    "    alpha,\n",
    "    precision10 / totalUsers,\n",
    "    mrrMetric / totalUsers,\n",
    "    mapMetric / totalUsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18716761\n"
     ]
    }
   ],
   "source": [
    "numEdges = 0\n",
    "for users in subredditIndexToUsers:\n",
    "    numEdges += len(users)\n",
    "print numEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.300087213516\n"
     ]
    }
   ],
   "source": [
    "# Test out timing here.\n",
    "start = time.time()\n",
    "currentSubreddit = 0\n",
    "rands = np.random.random((1000000,))\n",
    "for i in range(100000):\n",
    "    rand1 = int(len(subredditIndexToUsers[currentSubreddit]) * rands[i])\n",
    "    currentUser = subredditIndexToUsers[currentSubreddit][rand1]\n",
    "    rand2 = int(len(userIndexToSubreddits[currentUser]) * rands[10000 + i])\n",
    "    currentSubreddit = userIndexToSubreddits[currentUser][rand2]\n",
    "    # currentUser = np.random.choice(subredditIndexToUsers[currentSubreddit])\n",
    "    # currentSubreddit = np.random.choice(userIndexToSubreddits[currentUser])\n",
    "print time.time() - start\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'t5_2sfmf': 4, 't5_2th52': 7, 't5_2qzb6': 2, 't5_2qh33': 6, 't5_2rbod': 16, 't5_2qmg3': 17, 't5_2s28b': 6, 't5_2qjfk': 3, 't5_2qjyy': 37, 't5_2qh03': 3, 't5_37mcm': 1, 't5_3j2oi': 4, 't5_2rebv': 22, 't5_2qh1u': 2, 't5_2qh3u': 26, 't5_2x5s1': 2, 't5_2r8rv': 4, 't5_2qgzy': 19, 't5_2tk95': 1, 't5_3itci': 15, 't5_2qh1e': 2, 't5_2s5oq': 2, 't5_2si5v': 3, 't5_2s4yk': 4, 't5_2qh3l': 8, 't5_2qh1i': 15, 't5_3kfwf': 2, 't5_2s91l': 2, 't5_2qt55': 7, 't5_39usd': 20, 't5_2qqjc': 6, 't5_2qi6d': 3, 't5_2cneq': 44, 't5_2qh6e': 1, 't5_32xq7': 1, 't5_2qmah': 2, 't5_2qnts': 4, 't5_2qh3s': 6, 't5_3f5iq': 6, 't5_2qh0u': 8, 't5_34em3': 5, 't5_2qh13': 5, 't5_2qig7': 35, 't5_mouw': 4, 't5_2qxqc': 2, 't5_2rjpr': 25, 't5_21of': 1, 't5_2t160': 7, 't5_3fuwj': 9, 't5_2qkg3': 1, 't5_35g9w': 1, 't5_2qhx4': 1, 't5_2sokd': 1, 't5_2x66f': 9, 't5_3a1d4': 1})\n"
     ]
    }
   ],
   "source": [
    "print inputGraph[indexToUserId[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000\n",
      "Processed 2000000\n"
     ]
    }
   ],
   "source": [
    "actualNewSubreddits = tools.getUserIdToSubredditsByType(\"../bigData/finalGeneration/actualNewSubreddits\", \"newSubreddits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6797506\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for _, newSubreddits in actualNewSubreddits.iteritems():\n",
    "    total += len(newSubreddits)\n",
    "print total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subreddit id to name\n"
     ]
    }
   ],
   "source": [
    "print \"Loading subreddit id to name\"\n",
    "subredditIdToName = tools.read_subreddit_names(\"../bigData/subredditIdToName\")\n",
    "subredditNameToId = {node_id: node for node, node_id in subredditIdToName.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1000000\n",
      "Jloi2001\n",
      "User 2000000\n",
      "User 3000000\n",
      "ArchaicArmaldo\n",
      "User 4000000\n"
     ]
    }
   ],
   "source": [
    "subredditNames = ['ShinyPokemon', 'PokemonPlaza', 'pokemon', 'PokemonCreate', 'CasualPokemonTrades', 'battleagency', 'pokemontrades', 'PokeMoonSun', 'SVExchange', 'friendsafari', 'FestivalPlaza', 'pokemonshowdown', 'pokemonrng', 'ANormalDayintheSims', 'stunfisk', 'childtheories', 'Pokemonexchange', 'OrbitingSphere', 'Pokemongiveaway', 'SourBuizel', 'richarddawkins', 'watermelon', 'historyboners', 'PokemonSunAndMoon', 'LemonySnicket', 'ConfessionBear', 'detectivepikachu', 'pokemonribbons', 'neildegrassetyson', 'WonderTrade', 'petpeeve', 'GrindsMyGears', 'PokemonRejuvenation', 'powersaves', 'PokemonFusion', 'PokeTube', 'bravewilderness', 'kidfree', 'powersaves3ds', 'GodlessWomen', 'pokespe', 'PokemonPokedexHelp', 'letterswap', 'rosesarered', 'pokemonexchangeref', 'Pokemon_Giveaways', 'Cookies', 'Pokemonguide', 'planetarysociety', 'sens', 'pokemonteams', 'PokemonConquest', 'pokemonXY', 'PokemonORAS', 'SoloPokes', 'pokemondraftleague', 'Winniethepooh', 'Muffins', 'twom', 'Ambipom', 'friendsafaricodes']\n",
    "subredditIds = [subredditNameToId[subredditName] for subredditName in subredditNames]\n",
    "for i, (userId, oldSubreddits) in enumerate(inputGraph.iteritems(), 1):\n",
    "    if userId not in actualNewSubreddits:\n",
    "        continue\n",
    "    newSubreddits = actualNewSubreddits[userId]\n",
    "    if len(newSubreddits) > 100:\n",
    "        continue\n",
    "    \n",
    "    numTotal = len(subredditIds)\n",
    "    numInOld = 0\n",
    "    numInNew = 0\n",
    "    for subredditId in subredditIds:\n",
    "        if subredditId in oldSubreddits:\n",
    "            numInOld += 1\n",
    "        if subredditId in newSubreddits:\n",
    "            numInNew += 1\n",
    "    if numInOld >= 4 and numInNew >= 3:\n",
    "        print userId\n",
    "    if i % 1000000 == 0:\n",
    "        print \"User {}\".format(i)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CasualPokemonTrades', 'PokemonPlaza', 'Pokemongiveaway', 'pokemontrades', 'pokemon']\n",
      "['SVExchange', 'relaxedpokemontrades', 'Pokemonexchange', 'pokemonrng', 'PokeMoonSun']\n"
     ]
    }
   ],
   "source": [
    "print [subredditIdToName[thing] for thing in inputGraph[\"Jloi2001\"]]\n",
    "print [subredditIdToName[thing] for thing in actualNewSubreddits[\"Jloi2001\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../bigData/finalGeneration/user1', 'w') as outfile:\n",
    "    for userId in [\"Jloi2001\"]:\n",
    "        userJson = {\"userId\": userId, \n",
    "                    \"newSubreddits\": actualNewSubreddits[userId],\n",
    "                    \"oldSubreddits\": inputGraph[userId]}\n",
    "        outfile.write(json.dumps(userJson) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PokeMoonSun  AskReddit  PokemonCreate  ShinyPokemon  SVExchange  gaming  friendsafari  Pokemonexchange  battleagency  stunfisk \n"
     ]
    }
   ],
   "source": [
    "a = ['t5_3csff', 't5_2qh1i', 't5_3ggz0', 't5_2se9w', 't5_2z47n', 't5_2qh03', 't5_2yt52', 't5_2ukac', 't5_4bnoh', 't5_2sn6d']\n",
    "b = [subredditIdToName[thing] for thing in a]\n",
    "line = \"\"\n",
    "for thing in b:\n",
    "    line += \" {} \".format(thing)\n",
    "print line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
